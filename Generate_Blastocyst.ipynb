{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Drive"
      ],
      "metadata": {
        "id": "JRscpwGlVd_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3DXLL4VXK-6",
        "outputId": "42d95e6d-5586-4826-fa24-0249f3d5af4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Necessary Packages"
      ],
      "metadata": {
        "id": "O9BN-hxBWGXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LPIPS is used to measure the perceptual similarity between two images.\n",
        "# For calculating perceptual hashes of images, which are useful for identifying similar or duplicate images\n",
        "!pip -q install lpips imagehash"
      ],
      "metadata": {
        "id": "3WwOUW6DWK7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import spectral_norm as SN\n",
        "from torch.nn.utils import weight_norm\n",
        "import math\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "class PixelNorm(nn.Module):\n",
        "    \"\"\"Channel‑wise ℓ2 normalisation (StyleGAN).\"\"\"\n",
        "\n",
        "    def forward(self, x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:  # type: ignore[override]\n",
        "        return x / torch.sqrt(torch.mean(x ** 2, dim=1, keepdim=True) + eps)\n",
        "\n",
        "\n",
        "class NoiseInjection(nn.Module):\n",
        "    \"\"\"Adds learnable per‑channel scalar × noise map (identical across batch).\"\"\"\n",
        "\n",
        "    def __init__(self, channels: int):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.zeros(1, channels, 1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, noise: Optional[torch.Tensor] = None) -> torch.Tensor:  # type: ignore[override]\n",
        "        if noise is None:\n",
        "            noise = torch.randn(x.size(0), 1, x.size(2), x.size(3), device=x.device, dtype=x.dtype)\n",
        "        return x + self.weight * noise\n",
        "\n",
        "\n",
        "# 3×3 blur kernel (StyleGAN2)\n",
        "_blur_kernel = torch.tensor([1, 2, 1], dtype=torch.float32)\n",
        "_blur_kernel = (_blur_kernel[:, None] * _blur_kernel[None, :]) / _blur_kernel.sum()\n",
        "\n",
        "\n",
        "def blur(x: torch.Tensor) -> torch.Tensor:\n",
        "    k = _blur_kernel.to(x.device, x.dtype).repeat(x.size(1), 1, 1, 1)\n",
        "    return F.conv2d(x, k, padding=1, groups=x.size(1))\n",
        "\n",
        "\n",
        "class SelfAttn2d(nn.Module):\n",
        "    \"\"\"Non‑local self‑attention (SAGAN).\"\"\"\n",
        "\n",
        "    def __init__(self, in_ch: int) -> None:\n",
        "        super().__init__()\n",
        "        self.q = SN(nn.Conv1d(in_ch, in_ch // 8, 1, bias=False))\n",
        "        self.k = SN(nn.Conv1d(in_ch, in_ch // 8, 1, bias=False))\n",
        "        self.v = SN(nn.Conv1d(in_ch, in_ch, 1, bias=False))\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
        "        b, c, h, w = x.size()\n",
        "        flat = x.view(b, c, -1)\n",
        "        attn = self.q(flat).permute(0, 2, 1) @ self.k(flat)\n",
        "        attn = F.softmax(attn / math.sqrt(c / 8), dim=-1)\n",
        "        out = self.v(flat) @ attn.permute(0, 2, 1)\n",
        "        return x + self.gamma * out.view(b, c, h, w)\n",
        "\n",
        "\n",
        "class GResBlock(nn.Module):\n",
        "    \"\"\"Upsample → blur → conv×2 + skip (StyleGAN2 style).\"\"\"\n",
        "\n",
        "    def __init__(self, in_ch: int, out_ch: int):\n",
        "        super().__init__()\n",
        "        self.skip = nn.Conv2d(in_ch, out_ch, 1, bias=False)\n",
        "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "        self.noise1, self.noise2 = NoiseInjection(out_ch), NoiseInjection(out_ch)\n",
        "        self.pn1, self.pn2 = PixelNorm(), PixelNorm()\n",
        "        for m in (self.conv1, self.conv2, self.skip):\n",
        "            nn.init.kaiming_normal_(m.weight, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
        "        y = blur(F.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
        "        y = F.leaky_relu(self.pn1(self.noise1(self.conv1(y))), 0.2, inplace=True)\n",
        "        y = F.leaky_relu(self.pn2(self.noise2(self.conv2(y))), 0.2, inplace=True)\n",
        "        skip = self.skip(F.interpolate(x, scale_factor=2, mode=\"nearest\"))\n",
        "        return (y + skip) * (1 / math.sqrt(2))\n",
        "\n",
        "\n",
        "class DResBlock(nn.Module):\n",
        "    \"\"\"Downsample residual block with spectral‑norm convs.\"\"\"\n",
        "\n",
        "    def __init__(self, in_ch: int, out_ch: int):\n",
        "        super().__init__()\n",
        "        self.conv1, self.conv2 = SN(nn.Conv2d(in_ch, out_ch, 3, padding=1)), SN(nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        self.skip = SN(nn.Conv2d(in_ch, out_ch, 1, bias=False))\n",
        "        self.avg_pool = nn.AvgPool2d(2)\n",
        "        for m in (self.conv1, self.conv2, self.skip):\n",
        "            nn.init.kaiming_normal_(m.weight, a=0, mode=\"fan_in\", nonlinearity=\"leaky_relu\")\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:  # type: ignore[override]\n",
        "        y = F.leaky_relu(self.conv1(x), 0.2, inplace=True)\n",
        "        y = F.leaky_relu(self.conv2(y), 0.2, inplace=True)\n",
        "        y = self.avg_pool(y)\n",
        "        return (y + self.avg_pool(self.skip(x))) * (1 / math.sqrt(2))\n",
        "\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"256×256 conditional generator with clamped to-RGB dynamic range.\"\"\"\n",
        "    def __init__(self,\n",
        "                 z_dim: int = 128,\n",
        "                 label_dim: int = 50,\n",
        "                 base_ch: int = 1024,\n",
        "                 num_cls: int = 4):\n",
        "        super().__init__()\n",
        "\n",
        "        # ---------------- label embeddings -----------------\n",
        "        self.exp_embed = nn.Embedding(num_cls + 1, label_dim)   # EXP 0-5\n",
        "        self.icm_embed = nn.Embedding(num_cls,     label_dim)   # ICM A-C\n",
        "        self.te_embed  = nn.Embedding(num_cls,     label_dim)   # TE  A-C\n",
        "        lbl_tot = 3 * label_dim\n",
        "\n",
        "        # ---------------- latent → 4×4 ---------------------\n",
        "        self.fc      = nn.Linear(z_dim + lbl_tot, base_ch * 4 * 4)\n",
        "        self.lbl_fc4 = nn.Linear(lbl_tot,          base_ch * 4 * 4)\n",
        "\n",
        "        # ---------------- upsample backbone ----------------\n",
        "        self.b8   = GResBlock(base_ch,       base_ch // 2)   # 4→8\n",
        "        self.b16  = GResBlock(base_ch // 2,  base_ch // 4)   # 8→16\n",
        "        self.b32  = GResBlock(base_ch // 4,  base_ch // 8)   # 16→32\n",
        "        self.att32 = SelfAttn2d(base_ch // 8)\n",
        "        self.b64  = GResBlock(base_ch // 8,  base_ch // 16)  # 32→64\n",
        "        self.b128 = GResBlock(base_ch // 16, base_ch // 32)  # 64→128\n",
        "        self.b256 = GResBlock(base_ch // 32, base_ch // 64)  # 128→256\n",
        "\n",
        "        # ---------------- clamped to-RGB -------------------\n",
        "        # weight-norm separates direction (v) & magnitude (g)\n",
        "        self.to_rgb = weight_norm(nn.Conv2d(base_ch // 64, 3, 3, padding=1))\n",
        "\n",
        "        # initialise direction + *small* gain\n",
        "        nn.init.normal_(self.to_rgb.weight_v, 0.0, 1.0)\n",
        "        nn.init.normal_(self.to_rgb.weight_g, 0.0, 0.02)\n",
        "\n",
        "        # clamp g *every* forward pass (pre-hook)\n",
        "        def _clamp_gain(_, inp):\n",
        "            self.to_rgb.weight_g.data.clamp_(0.0, 0.05)\n",
        "        self.to_rgb.register_forward_pre_hook(_clamp_gain)\n",
        "\n",
        "        nn.init.zeros_(self.to_rgb.bias)\n",
        "\n",
        "    # -------------------------------------------------------\n",
        "    def _lbl_vec(self, exp, icm, te):\n",
        "        return torch.cat([self.exp_embed(exp),\n",
        "                          self.icm_embed(icm),\n",
        "                          self.te_embed(te)], dim=1)\n",
        "\n",
        "    def forward(self, z, exp, icm, te):\n",
        "        \"\"\"\n",
        "        z   : [B, z_dim]\n",
        "        exp : [B]   expansion grade (0-5, 0 = unexpanded)\n",
        "        icm : [B]   ICM grade (0=A,1=B,2=C,3=D if present)\n",
        "        te  : [B]   TE grade  (0=A,1=B,2=C,3=D)\n",
        "        \"\"\"\n",
        "        lbl = self._lbl_vec(exp, icm, te)                         # [B,150]\n",
        "        x = self.fc(torch.cat([z, lbl], dim=1))                   # [B,1024*4*4]\n",
        "        x = x.view(-1, 1024, 4, 4)\n",
        "        x = x + self.lbl_fc4(lbl).view_as(x)\n",
        "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
        "\n",
        "        x = self.b8(x)\n",
        "        x = self.b16(x)\n",
        "        x = self.b32(x)\n",
        "        x = self.att32(x)\n",
        "        x = self.b64(x)\n",
        "        x = self.b128(x)\n",
        "        x = self.b256(x)\n",
        "\n",
        "        rgb = self.to_rgb(x)                      # clamped gain here\n",
        "        return torch.tanh(rgb)                    # [-1,1] image"
      ],
      "metadata": {
        "id": "ih9ooOefWsNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZWceKWfU24g",
        "outputId": "0183451c-2399-4374-d726-cdde0cdf2faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model from: /usr/local/lib/python3.11/dist-packages/lpips/weights/v0.1/vgg.pth\n"
          ]
        }
      ],
      "source": [
        "# === 0. imports & helpers ======================================\n",
        "import torch, lpips, imagehash, PIL.Image as PIL\n",
        "from pathlib import Path\n",
        "from torchvision.utils import save_image\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "CKPT_FOLDER = \"/content/drive/MyDrive/Msc in AI/Deep Learning/Blastocyst_Dataset/cgan_checkpoints/\"\n",
        "\n",
        "device      = \"cuda\"\n",
        "z_dim       = 128\n",
        "batch       = 32\n",
        "checkpoints = [             # pick 3–5 diverse ones\n",
        "    CKPT_FOLDER+\"gan_epoch_825.pth\",\n",
        "    CKPT_FOLDER+\"gan_epoch_850.pth\",\n",
        "    CKPT_FOLDER+\"gan_epoch_875.pth\",\n",
        "    CKPT_FOLDER+\"gan_epoch_900.pth\"\n",
        "]\n",
        "psi_trunc   = 0.8           # truncation trick for quality\n",
        "lpips_fn    = lpips.LPIPS(net='vgg').to(device).eval()\n",
        "\n",
        "# ---- simple dup filter (phash + LPIPS) ------------------------\n",
        "def is_novel(img, seen_hashes, thresh_lpips=0.7):\n",
        "    h = imagehash.phash(img)\n",
        "    # 1) hash quick-reject\n",
        "    if any(h - h0 <= 4 for h0 in seen_hashes):          #≤4 bits diff\n",
        "        # 2) LPIPS exact check\n",
        "        img_t = torch.from_numpy(np.array(img).transpose(2,0,1))\\\n",
        "                    .float().div(255).unsqueeze(0).to(device)\n",
        "        for cand in seen_hashes[h]:                      # list of PILs\n",
        "            cand_t = torch.from_numpy(np.array(cand).transpose(2,0,1))\\\n",
        "                        .float().div(255).unsqueeze(0).to(device)\n",
        "            d = lpips_fn(img_t*2-1, cand_t*2-1).item()\n",
        "            if d < thresh_lpips:\n",
        "                return False\n",
        "    return True\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch, random, itertools\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/Msc in AI/Deep Learning/Blastocyst_Dataset/Gardner_train_silver.csv\"          # ← change if necessary\n",
        "df = pd.read_csv(CSV_PATH, delimiter=\";\")\n",
        "\n",
        "# The CSV is assumed to have columns like 'EXP', 'ICM', 'TE'\n",
        "triples, freqs = np.unique(\n",
        "    df[[\"EXP_silver\", \"ICM_silver\", \"TE_silver\"]].astype(int).values, axis=0, return_counts=True\n",
        ")\n",
        "\n",
        "valid_triples = [tuple(map(int, t)) for t in triples]\n",
        "freqs         = freqs / freqs.sum()          # normalise for probability\n",
        "\n",
        "class TripleSampler:\n",
        "    \"\"\"\n",
        "    Draw (exp, icm, te) ONLY from the list supplied in `valid_triples`.\n",
        "    If `weighted=True` the empirical frequency in the CSV is respected.\n",
        "    \"\"\"\n",
        "    def __init__(self, triples, probs=None, device=\"cpu\"):\n",
        "        self.device  = device\n",
        "        self.triples = torch.tensor(triples, device=device)\n",
        "        if probs is None:\n",
        "            self.probs = torch.ones(len(triples), device=device) / len(triples)\n",
        "        else:\n",
        "            self.probs = torch.tensor(probs, device=device)\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        idx  = torch.multinomial(self.probs, batch, replacement=True)\n",
        "        pick = self.triples[idx]                     # [B,3]\n",
        "        return pick[:,0], pick[:,1], pick[:,2]       # exp, icm, te\n",
        "\n",
        "valid_sampler = TripleSampler(valid_triples, freqs, device=device)"
      ],
      "metadata": {
        "id": "SJRb2inw687C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 1. target counters ========================================\n",
        "targets = {\n",
        "    \"EXP\": {k: 1000 for k in range(5)},\n",
        "    \"ICM\": {k: 1000 for k in range(4)},       # 0:A,1:B,2:C\n",
        "    \"TE\" : {k: 1000 for k in range(4)}\n",
        "}\n",
        "out_root = Path(\"/content/drive/MyDrive/Msc in AI/Deep Learning/Blastocyst_Dataset/Synthetic_GAN\"); out_root.mkdir(exist_ok=True, parents=True)\n",
        "for cat,d in targets.items():\n",
        "    for lab in d: (out_root/cat/str(lab)).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "seen_hash = {}          # {hash : [PILs]} for dup filter\n"
      ],
      "metadata": {
        "id": "an4AKy1WVaTO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === 2. harvest loop ===========================================\n",
        "for ckpt in checkpoints:\n",
        "    G = Generator(z_dim=z_dim).to(device)\n",
        "    G.load_state_dict(torch.load(ckpt, map_location=\"cpu\")[\"G\"])\n",
        "    G.eval()\n",
        "\n",
        "    while any(v>0 for cat in targets.values() for v in cat.values()):\n",
        "        z  = torch.randn(batch, z_dim, device=device) * psi_trunc\n",
        "        exp, icm, te = valid_sampler(batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            imgs = (G(z, exp, icm, te) + 1) / 2     # [0,1]\n",
        "\n",
        "        for k,(img_t,e,i,t) in enumerate(zip(imgs, exp, icm, te)):\n",
        "            # ------------- EXP bucket ---------------------------\n",
        "            if targets[\"EXP\"][e.item()] > 0:\n",
        "                pil = PIL.fromarray((img_t.mul(255)\n",
        "                                    .byte().permute(1,2,0)\n",
        "                                    .cpu().numpy()))\n",
        "                if is_novel(pil, seen_hash):\n",
        "                    fname = out_root/\"EXP\"/str(e.item())/f\"{targets['EXP'][e.item()]:04d}.png\"\n",
        "                    pil.save(fname)\n",
        "                    targets[\"EXP\"][e.item()] -= 1\n",
        "            # ------------- ICM bucket ---------------------------\n",
        "            label_i = i.item()\n",
        "            if targets[\"ICM\"][label_i] > 0:\n",
        "                pil = PIL.fromarray((img_t.mul(255).byte()\n",
        "                                    .permute(1,2,0).cpu().numpy()))\n",
        "                if is_novel(pil, seen_hash):\n",
        "                    fname = out_root/\"ICM\"/str(label_i)/f\"{targets['ICM'][label_i]:04d}.png\"\n",
        "                    pil.save(fname)\n",
        "                    targets[\"ICM\"][label_i] -= 1\n",
        "            # ------------- TE bucket ----------------------------\n",
        "            label_t = t.item()\n",
        "            if targets[\"TE\"][label_t] > 0:\n",
        "                pil = PIL.fromarray((img_t.mul(255).byte()\n",
        "                                    .permute(1,2,0).cpu().numpy()))\n",
        "                if is_novel(pil, seen_hash):\n",
        "                    fname = out_root/\"TE\"/str(label_t)/f\"{targets['TE'][label_t]:04d}.png\"\n",
        "                    pil.save(fname)\n",
        "                    targets[\"TE\"][label_t] -= 1\n",
        "\n",
        "        # optional progress print\n",
        "        if random.random()<.05:\n",
        "            print({k:sum(v.values()) for k,v in targets.items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0TlMlZfU_uu",
        "outputId": "e66ebcb4-f4e1-42a5-a41c-e39d91417bd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
            "  WeightNorm.apply(module, name, dim)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'EXP': 4520, 'ICM': 3520, 'TE': 3520}\n",
            "{'EXP': 3752, 'ICM': 2752, 'TE': 2752}\n",
            "{'EXP': 3112, 'ICM': 2305, 'TE': 2112}\n",
            "{'EXP': 3056, 'ICM': 2281, 'TE': 2057}\n",
            "{'EXP': 2686, 'ICM': 1974, 'TE': 1657}\n",
            "{'EXP': 1315, 'ICM': 952, 'TE': 880}\n",
            "{'EXP': 1176, 'ICM': 947, 'TE': 867}\n",
            "{'EXP': 786, 'ICM': 929, 'TE': 838}\n",
            "{'EXP': 699, 'ICM': 926, 'TE': 833}\n",
            "{'EXP': 693, 'ICM': 926, 'TE': 831}\n",
            "{'EXP': 640, 'ICM': 925, 'TE': 828}\n",
            "{'EXP': 570, 'ICM': 920, 'TE': 820}\n",
            "{'EXP': 272, 'ICM': 905, 'TE': 779}\n",
            "{'EXP': 217, 'ICM': 899, 'TE': 772}\n",
            "{'EXP': 198, 'ICM': 897, 'TE': 767}\n",
            "{'EXP': 150, 'ICM': 894, 'TE': 752}\n",
            "{'EXP': 113, 'ICM': 888, 'TE': 740}\n",
            "{'EXP': 110, 'ICM': 888, 'TE': 739}\n",
            "{'EXP': 80, 'ICM': 886, 'TE': 729}\n",
            "{'EXP': 34, 'ICM': 880, 'TE': 709}\n",
            "{'EXP': 0, 'ICM': 878, 'TE': 687}\n",
            "{'EXP': 0, 'ICM': 876, 'TE': 682}\n",
            "{'EXP': 0, 'ICM': 876, 'TE': 680}\n",
            "{'EXP': 0, 'ICM': 874, 'TE': 678}\n",
            "{'EXP': 0, 'ICM': 874, 'TE': 676}\n",
            "{'EXP': 0, 'ICM': 871, 'TE': 660}\n",
            "{'EXP': 0, 'ICM': 870, 'TE': 658}\n",
            "{'EXP': 0, 'ICM': 868, 'TE': 656}\n",
            "{'EXP': 0, 'ICM': 867, 'TE': 653}\n",
            "{'EXP': 0, 'ICM': 863, 'TE': 648}\n",
            "{'EXP': 0, 'ICM': 863, 'TE': 648}\n",
            "{'EXP': 0, 'ICM': 861, 'TE': 641}\n",
            "{'EXP': 0, 'ICM': 859, 'TE': 640}\n",
            "{'EXP': 0, 'ICM': 856, 'TE': 633}\n",
            "{'EXP': 0, 'ICM': 856, 'TE': 633}\n",
            "{'EXP': 0, 'ICM': 842, 'TE': 600}\n",
            "{'EXP': 0, 'ICM': 841, 'TE': 600}\n",
            "{'EXP': 0, 'ICM': 833, 'TE': 581}\n",
            "{'EXP': 0, 'ICM': 829, 'TE': 567}\n",
            "{'EXP': 0, 'ICM': 828, 'TE': 558}\n",
            "{'EXP': 0, 'ICM': 820, 'TE': 541}\n",
            "{'EXP': 0, 'ICM': 811, 'TE': 501}\n",
            "{'EXP': 0, 'ICM': 807, 'TE': 481}\n",
            "{'EXP': 0, 'ICM': 803, 'TE': 471}\n",
            "{'EXP': 0, 'ICM': 796, 'TE': 442}\n",
            "{'EXP': 0, 'ICM': 777, 'TE': 396}\n",
            "{'EXP': 0, 'ICM': 760, 'TE': 370}\n",
            "{'EXP': 0, 'ICM': 738, 'TE': 303}\n",
            "{'EXP': 0, 'ICM': 725, 'TE': 265}\n",
            "{'EXP': 0, 'ICM': 723, 'TE': 260}\n",
            "{'EXP': 0, 'ICM': 722, 'TE': 256}\n",
            "{'EXP': 0, 'ICM': 722, 'TE': 247}\n",
            "{'EXP': 0, 'ICM': 720, 'TE': 240}\n",
            "{'EXP': 0, 'ICM': 706, 'TE': 214}\n",
            "{'EXP': 0, 'ICM': 705, 'TE': 214}\n",
            "{'EXP': 0, 'ICM': 697, 'TE': 196}\n",
            "{'EXP': 0, 'ICM': 693, 'TE': 178}\n",
            "{'EXP': 0, 'ICM': 685, 'TE': 149}\n",
            "{'EXP': 0, 'ICM': 681, 'TE': 146}\n",
            "{'EXP': 0, 'ICM': 678, 'TE': 136}\n",
            "{'EXP': 0, 'ICM': 678, 'TE': 134}\n",
            "{'EXP': 0, 'ICM': 676, 'TE': 125}\n",
            "{'EXP': 0, 'ICM': 673, 'TE': 111}\n",
            "{'EXP': 0, 'ICM': 669, 'TE': 101}\n",
            "{'EXP': 0, 'ICM': 669, 'TE': 101}\n",
            "{'EXP': 0, 'ICM': 669, 'TE': 100}\n",
            "{'EXP': 0, 'ICM': 664, 'TE': 94}\n",
            "{'EXP': 0, 'ICM': 662, 'TE': 93}\n",
            "{'EXP': 0, 'ICM': 661, 'TE': 87}\n",
            "{'EXP': 0, 'ICM': 661, 'TE': 87}\n",
            "{'EXP': 0, 'ICM': 656, 'TE': 74}\n",
            "{'EXP': 0, 'ICM': 656, 'TE': 74}\n",
            "{'EXP': 0, 'ICM': 655, 'TE': 71}\n",
            "{'EXP': 0, 'ICM': 639, 'TE': 18}\n",
            "{'EXP': 0, 'ICM': 634, 'TE': 10}\n",
            "{'EXP': 0, 'ICM': 634, 'TE': 10}\n",
            "{'EXP': 0, 'ICM': 630, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 630, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 627, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 622, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 621, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 613, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 600, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 595, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 592, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 589, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 586, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 583, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 576, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 574, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 566, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 562, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 562, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 561, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 556, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 545, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 545, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 545, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 542, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 537, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 537, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 525, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 524, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 510, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 507, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 489, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 487, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 480, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 478, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 478, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 477, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 476, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 476, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 475, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 469, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 459, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 454, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 452, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 452, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 438, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 437, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 434, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 429, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 429, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 428, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 418, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 418, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 411, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 407, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 405, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 401, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 397, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 393, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 393, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 390, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 389, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 380, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 379, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 375, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 373, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 363, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 363, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 359, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 354, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 347, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 344, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 338, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 336, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 326, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 313, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 303, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 302, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 296, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 296, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 295, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 290, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 283, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 280, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 279, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 279, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 276, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 275, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 274, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 272, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 271, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 242, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 240, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 239, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 234, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 231, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 229, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 228, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 227, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 227, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 217, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 205, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 205, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 205, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 205, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 201, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 200, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 194, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 193, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 188, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 185, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 183, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 183, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 180, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 180, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 177, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 174, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 172, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 167, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 163, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 161, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 154, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 147, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 140, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 131, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 130, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 129, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 128, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 113, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 100, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 97, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 95, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 86, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 86, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 84, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 69, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 68, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 60, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 58, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 57, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 56, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 56, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 49, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 42, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 40, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 37, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 28, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 19, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 12, 'TE': 0}\n",
            "{'EXP': 0, 'ICM': 1, 'TE': 0}\n"
          ]
        }
      ]
    }
  ]
}